{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperion Capstone 7\n",
    "\n",
    "This project will focus on comparing the [K-mean](https://en.wikipedia.org/wiki/K-means_clustering) and the [KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) methods of clustering data after applying Principle Componant Analysis. The dataset used in this notebook will be [US arrest statistics](https://www.kaggle.com/datasets/kurohana/usarrets). This dataset is described as; Arrests per 100,000 residents for assult, murder, and rape in each of the 50 US states in 1973.\n",
    "\n",
    "The project strucutre:\n",
    "- A small EDA on the dataset\n",
    "- Applying PCA and tuning the number of principle componants\n",
    "- Training a K-mean model\n",
    "- Training a KNN model\n",
    "- Comparing both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from itertools import combinations\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and exploring data\n",
    "\n",
    "In this section I will be looking at the data set and looking at the different attributes and how they link together. I will also be dealing with any empty values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "df = pd.read_csv(\"UsArrests.csv\")\n",
    "\n",
    "#Checking for empty cells\n",
    "df.info()\n",
    "\n",
    "#Gets the first 5 rows\n",
    "print(\"\\n\\nTop of the dataset:\\n\",df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initial opening of the dataset tells us that there are no empty cells that need to be dealt with. It also seems that the attribute names \"City\" is named wrong and should be \"State\" instead. The next step will be analysing the distribution of each statistic throughout the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changes name of the \"City\" column\n",
    "df.rename(columns={\"City\":\"State\"},inplace=True)\n",
    "\n",
    "#Dict to convert US state names to abbreviations\n",
    "us_state_to_abbrev = {\n",
    "\"Alabama\": \"AL\",\n",
    "\"Alaska\": \"AK\",\n",
    "\"Arizona\": \"AZ\",\n",
    "\"Arkansas\": \"AR\",\n",
    "\"California\": \"CA\",\n",
    "\"Colorado\": \"CO\",\n",
    "\"Connecticut\": \"CT\",\n",
    "\"Delaware\": \"DE\",\n",
    "\"Florida\": \"FL\",\n",
    "\"Georgia\": \"GA\",\n",
    "\"Hawaii\": \"HI\",\n",
    "\"Idaho\": \"ID\",\n",
    "\"Illinois\": \"IL\",\n",
    "\"Indiana\": \"IN\",\n",
    "\"Iowa\": \"IA\",\n",
    "\"Kansas\": \"KS\",\n",
    "\"Kentucky\": \"KY\",\n",
    "\"Louisiana\": \"LA\",\n",
    "\"Maine\": \"ME\",\n",
    "\"Maryland\": \"MD\",\n",
    "\"Massachusetts\": \"MA\",\n",
    "\"Michigan\": \"MI\",\n",
    "\"Minnesota\": \"MN\",\n",
    "\"Mississippi\": \"MS\",\n",
    "\"Missouri\": \"MO\",\n",
    "\"Montana\": \"MT\",\n",
    "\"Nebraska\": \"NE\",\n",
    "\"Nevada\": \"NV\",\n",
    "\"New Hampshire\": \"NH\",\n",
    "\"New Jersey\": \"NJ\",\n",
    "\"New Mexico\": \"NM\",\n",
    "\"New York\": \"NY\",\n",
    "\"North Carolina\": \"NC\",\n",
    "\"North Dakota\": \"ND\",\n",
    "\"Ohio\": \"OH\",\n",
    "\"Oklahoma\": \"OK\",\n",
    "\"Oregon\": \"OR\",\n",
    "\"Pennsylvania\": \"PA\",\n",
    "\"Rhode Island\": \"RI\",\n",
    "\"South Carolina\": \"SC\",\n",
    "\"South Dakota\": \"SD\",\n",
    "\"Tennessee\": \"TN\",\n",
    "\"Texas\": \"TX\",\n",
    "\"Utah\": \"UT\",\n",
    "\"Vermont\": \"VT\",\n",
    "\"Virginia\": \"VA\",\n",
    "\"Washington\": \"WA\",\n",
    "\"West Virginia\": \"WV\",\n",
    "\"Wisconsin\": \"WI\",\n",
    "\"Wyoming\": \"WY\",\n",
    "\"District of Columbia\": \"DC\",\n",
    "\"American Samoa\": \"AS\",\n",
    "\"Guam\": \"GU\",\n",
    "\"Northern Mariana Islands\": \"MP\",\n",
    "\"Puerto Rico\": \"PR\",\n",
    "\"United States Minor Outlying Islands\": \"UM\",\n",
    "\"U.S. Virgin Islands\": \"VI\",\n",
    "}\n",
    "\n",
    "df.State = df.State.map(us_state_to_abbrev)\n",
    "\n",
    "#Selects subset only containing numeric data\n",
    "df_num = df.drop(\"State\",axis=1).set_index(df.State.values)\n",
    "\n",
    "#Generates subplots\n",
    "fig,axs = plt.subplots(1,4,figsize=(20,5))\n",
    "\n",
    "#Plots a histogram of each numeric column\n",
    "for col,ax in zip(df_num,fig.axes):\n",
    "    df[col].plot.hist(ax=ax)\n",
    "    ax.set_xlabel(col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Murder per Capita**\n",
    "- The most common rate of murder is around 2.5 and 7.5.\n",
    "- Rates above 11 are much rarer.\n",
    "\n",
    "**Assult per Capita**\n",
    "- The rates have a similar shape to the murder rates, but the absolute values are much higher.\n",
    "- Peak at 125, though there is another peak at 275.\n",
    "\n",
    "**Urban Population**\n",
    "- Most states have an urban population of around 65%.\n",
    "- There seems to be a much more even spread of populations between 45% and 95% compared to the other statistic types.\n",
    "\n",
    "**Rape per Capita**\n",
    "- More rape occures in general than murders\n",
    "- More assults then rapes\n",
    "- There a rapid dropoff for rates above 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The next step will be assessing if the dataset is a good fit for the application of PCA. A dataset that is a good candidate for PCA is one where there is strong positive or negative correlations between multiple columns. Meaning that linking these columns together could make the dataset easier to visualise (since there will be less than 4 columns), while not also losing a substantial amount of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the correlation between each attribute with every other attribute\n",
    "num_corr = df_num.corr()\n",
    "\n",
    "#Plots correlation grid\n",
    "sns.heatmap(num_corr,annot=True,vmin=-1,vmax=1,cmap=\"RdBu\").set(title=\"Correlation Grid\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- None of the attributes shows a negative correlation.\n",
    "- Most show a strong positive correlation.\n",
    "- Murder rates and Urban population show the least correlation with a value of 0.07\n",
    "- Assault rate and Urban population also shows a weak positive correlation\n",
    "- Murder and assault rates have the highest correlation of 0.8\n",
    "\n",
    "Overall, most of the attributes shows some sort of correlation with another. Therefore, doing a PCA would be beneficial and may result in data that could be visualised. However, shown in the histograms above, the range of values for each column is very different. For instance, the Urban population is given in percentage, while all other columns are per 100,000. Even within the columns that are per 100,000, there is a difference in their range. For example, murder rates reach a high of below 20, while assault rates reach a high of above 300.\n",
    "\n",
    "This big difference in the columns will cause the PCA to favour the larger values which may skew the end result. For this reason, I will now normalise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale numric data and put into a new dataframe\n",
    "df_num_scaled = pd.DataFrame(StandardScaler().fit_transform(df_num),columns=df_num.columns,index=df_num.index)\n",
    "\n",
    "#Shows that the data has been scaled\n",
    "print(f\"{'Unscaled data':=^50}\")\n",
    "print(df_num.describe())\n",
    "print(f\"\\n{'Scaled data':=^50}\")\n",
    "print(df_num_scaled.describe())\n",
    "\n",
    "#Plots histogram of both datasets to verify scaling\n",
    "fig,axs = plt.subplots(2,2)\n",
    "fig.tight_layout()\n",
    "\n",
    "#Murder plots\n",
    "df_num.Murder.plot.hist(ax=axs[0,0])\n",
    "axs[0,0].set_title(\"Unscaled Murder\")\n",
    "df_num_scaled.Murder.plot.hist(ax=axs[1,0])\n",
    "axs[1,0].set_title(\"Scaled Murder\")\n",
    "\n",
    "#Assault plots\n",
    "df_num.Assault.plot.hist(ax=axs[0,1])\n",
    "axs[0,1].set_title(\"Unscaled Assault\")\n",
    "df_num_scaled.Assault.plot.hist(ax=axs[1,1])\n",
    "axs[1,1].set_title(\"Scaled Assault\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scaling the dataset, we can now see that all columns have a mean of 0, and a standard deviation of 1. This is futher shown in the histograms showing the unscaled and scaled data. Now both murder and assault have values that lie arounf -2 and 2, but the overall shape of the plots are conserved.\n",
    "\n",
    "Before carrying out the PCA, a little more exploration can be done on the scaled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the state with the highest and lowest statistic for each column\n",
    "for col in df_num:\n",
    "    top = df_num[col].idxmax()\n",
    "    bot = df_num[col].idxmin()\n",
    "    print(f\"{col}:\")\n",
    "    print(f\"Top: {top} | Bottom: {bot}\\n\")\n",
    "\n",
    "#Plots the top 5 states for each statistic\n",
    "fig,axs = plt.subplots(2,4,figsize=(20,5))\n",
    "fig.tight_layout(pad=5.5)\n",
    "for i,col in enumerate(df_num):\n",
    "    top_5 = df_num[col].sort_values(ascending=False).iloc[:5]\n",
    "    top_5.plot.bar(ax=axs[0,i],color=\"r\")\n",
    "    axs[0,i].set_title(col)\n",
    "    axs[0,i].tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "    bot_5 = df_num[col].sort_values(ascending=True).iloc[:5]\n",
    "    bot_5.plot.bar(ax=axs[1,i],color=\"g\")\n",
    "    axs[1,i].tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "axs[0,0].set_ylabel(\"Top 5\")\n",
    "axs[1,0].set_ylabel(\"Bottom 5\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the extremes of the dataset, we can say that North Dakota is one of the safer states, while also being one with the lowest urban population. The same can be said for Vermont. Overall, a lot of the bottom 5 states in each statistic come up multiple times in other statistics.\n",
    "\n",
    "On the other hand, this is not the case for the top 5. For instance, Florida only shows in in the Murder and Assault stats.\n",
    "\n",
    "It is interesting to note that Mississippi has one of the highest murder rates, but also one of the lowest percentage of urban population.\n",
    "\n",
    "## PCA\n",
    "\n",
    "This section will be dedicated to the PCA of the data. A comparison to the scaled and unscaled data will be shown to demonstrate the importance of scaling the data. After that, the ratio of explained variance will be used to determine the optimal number of columns needed to reduce the complexity of the dataset, while also losing the least amount of information.\n",
    "\n",
    "Once this analysis is done and the optimal number of columns if found, a clustering model will be applied to the data.\n",
    "\n",
    "The first step is to apply the PCA to the datasets and visualise it. The visualisation will be done using a biplot. The biplot will show the position of the datapoints relative to some selected principle axes given by the PCA. It will also show the direction and importance of the original axes on relation to those axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biplot(score,coeff,labels=None,points=None):\n",
    "    \"\"\"Plots a biplot that shows specific principle axes along with the datapoints\"\"\"\n",
    "\n",
    "    #Gets data points from pca transformed data\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "\n",
    "    #Gets number of prinicple axes\n",
    "    n = coeff.shape[0]\n",
    "\n",
    "    #Scaled datapoints\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "\n",
    "    fig, ax = plt.subplots()    \n",
    "\n",
    "    #Plots the dataponits wrt principle axes\n",
    "    ax.scatter(xs * scalex,ys * scaley,s=5)\n",
    "\n",
    "    #Labels the datapoints\n",
    "    for i in range(0,len(xs)):\n",
    "        txt = points[i]\n",
    "        ax.annotate(txt, (xs[i]* scalex, ys[i]* scaley))\n",
    "\n",
    "    #Draws oringinal axes wrt to new principle axes\n",
    "    for i in range(n):\n",
    "        ax.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n",
    "        if labels is None:\n",
    "            ax.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'green', ha = 'center', va = 'center')\n",
    "        else:\n",
    "            ax.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n",
    " \n",
    "    plt.grid()\n",
    "\n",
    "    # Adapted from: https://ostwalprasad.github.io/machine-learning/PCA-using-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates PCA object\n",
    "r = 10\n",
    "pca = PCA(random_state=r)\n",
    "\n",
    "#Applies PCA to unscaled data\n",
    "pca_unscaled_array = pca.fit_transform(df_num)\n",
    "pca_unscaled = pd.DataFrame(pca_unscaled_array,index=df_num.index)\n",
    "\n",
    "#Gets the principle axes components\n",
    "unscaled_comps = pca.components_\n",
    "\n",
    "#Plots biplot\n",
    "biplot(pca_unscaled_array[:,0:2],np.transpose(unscaled_comps[0:2,:]),df_num.columns,df_num.index)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This biplot of the unscaled data shows that the Urban population and Assault columns dominate the principle axes, this is shown by the length of the red arrows. This makes sense since compared to the other columns, their range of values is much larger. We ideally want all the oringial axes to have a similar influence on the principle axes. This can be fixed by scaling the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms scaled data via PCA\n",
    "pca = PCA()\n",
    "pca_scaled_array = pca.fit_transform(df_num_scaled)\n",
    "pca_scaled = pd.DataFrame(pca_scaled_array,index=df_num_scaled.index)\n",
    "\n",
    "#Gets principle axes components\n",
    "scaled_comps = pca.components_\n",
    "\n",
    "#Plots biplot using PC1,PC2\n",
    "biplot(pca_scaled_array[:,0:2],np.transpose(scaled_comps[0:2,:]),df_num_scaled.columns,df_num_scaled.index)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots biplot using PC1,PC2\n",
    "biplot(pca_scaled_array[:,[2,0]],np.transpose(scaled_comps[[2,0],:]),df_num_scaled.columns,df_num_scaled.index)\n",
    "plt.xlabel(\"PC3\")\n",
    "plt.ylabel(\"PC1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biplot(pca_scaled_array[:,[1,2]],np.transpose(scaled_comps[[1,2],:]),df_num_scaled.columns,df_num_scaled.index)\n",
    "plt.xlabel(\"PC2\")\n",
    "plt.ylabel(\"PC3\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have scaled the data, the biplot is much more even. Each of the original axes now have similar lengths and therefore similar importance levels.\n",
    "\n",
    "The biplot indicates that the first principle axes (PC1) represents some sort of general danger level of a state, since all 3 columns that represent some crime are all points along the PC1 axes. The 2nd principle axes seems to represent the general population distribution of the state, having states with larger urban populations have negative PC2 values.\n",
    "\n",
    "The next stage of the analysis to to determine the optimal number of columns. This is done by looking at how the explined variance changes depending on the number of principle axes used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots the cumulative explained variance and the abs explained varaince\n",
    "fig, axs = plt.subplots(2,sharex='col')\n",
    "\n",
    "axs[0].plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "axs[0].set_ylabel(\"Cumulative explained variance\")\n",
    "axs[1].plot(pca.explained_variance_ratio_)\n",
    "axs[1].set_ylabel(\"Explained variance\")\n",
    "axs[1].set_xlabel(\"Number of principle axes\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots show that the increase in explained variance is very low after the 3rd principle axis has been generated. However, the amount of explained variance is reduced significantly if we only have 2 principle axes or lower. This leads me to think that have 3 is the best choice. \n",
    "\n",
    "Another benefit of only having 3 principle axis is that a 3D plot can be visualised where the clustering may be more straightforward to understand once that has been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes the unwanted principle axis\n",
    "final_pca = pca_scaled.drop(3,axis=1)\n",
    "\n",
    "final_pca.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering - K-Mean\n",
    "\n",
    "In this part of the report, I will be using a k-mean model of unsupervised learning to try and organise my data into clusters.\n",
    "\n",
    "The first step will be to find the number of clusters that are optimum. This can be done by applying k-means to the dataset specifying a different number of clusters each time. The [silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering)) can then be found and plotted for each test. \n",
    "\n",
    "The silhouette score is a metric that is used to measure if the predicted clusters are well distinguished from eachother or too close together. A good clustering outcome would be one where the clusters are separated enough to be distinct from eachother. The score itself can be between -1 and 1.\n",
    "\n",
    "- 1: Clusters are well spread out\n",
    "- 0: Clusters are overlapping completely\n",
    "- -1: Clusters are predicted wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette Scores:\")\n",
    "\n",
    "#Applys K-mean to the data using a range of cluster options\n",
    "score = []\n",
    "for i in range(2,10):\n",
    "    model = KMeans(n_clusters=i,random_state=r)\n",
    "    model.fit(final_pca)\n",
    "    score.append(silhouette_score(final_pca,model.labels_))\n",
    "\n",
    "#Plots all the silhouette scores collected\n",
    "plt.plot(list(range(2,10)),score)\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Score\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette scores indicate that the best number of clusters should be 2. This is shown due to the fact that the score very quickly decreases if there are more than 2 clusters. Therefore, the clusters are becoming more and more similar, which is not what we want when we want clusters. \n",
    "\n",
    "Now we will run the clustering again but with only 2 clusters and visualise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trains and predicts using k-means to find 2 clusters\n",
    "kmean = KMeans(n_clusters=2,random_state=r)\n",
    "cluster_labels = [int(value) for value in kmean.fit_predict(final_pca)]\n",
    "\n",
    "#Separates data into 3 variables and the labels\n",
    "x = final_pca[0]\n",
    "y = final_pca[1]\n",
    "z = final_pca[2]\n",
    "labels = final_pca.index\n",
    "\n",
    "#Gets combinations of 3 principle axes\n",
    "combo = combinations(final_pca,2)\n",
    "\n",
    "#Plots the datapoints with thier relatvent clustering\n",
    "fig, axs = plt.subplots(1,3,figsize=(15,5))\n",
    "for axis,comp in zip(fig.axes,combo):\n",
    "    \n",
    "    #Gets the principle axis \n",
    "    comp = list(comp)\n",
    "    xs = comp[0]\n",
    "    ys = comp[1]\n",
    "\n",
    "    #Plots the scatter\n",
    "    final_pca[comp].plot.scatter(x=xs,y=ys,ax=axis,color=cluster_labels,cmap=matplotlib.colors.ListedColormap([\"red\",\"blue\"]))\n",
    "    axis.set_xlabel(f\"PC{comp[0]+1}\")\n",
    "    axis.set_ylabel(f\"PC{comp[1]+1}\")\n",
    "\n",
    "    #Label each point in the plot\n",
    "    for state in final_pca.index:\n",
    "        axis.annotate(state,(final_pca.loc[state,xs],final_pca.loc[state,ys]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Generates 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "\n",
    "#plots the scatter using the clustering results for colouring points\n",
    "ax.scatter(x,y,z,c=cluster_labels,cmap=\"RdBu\")\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_zlabel(\"PC3\")\n",
    "ax.view_init(20,85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initaise dataframe that will contain the importance of each original axis with each new principle axis\n",
    "importance = pd.DataFrame({\"Attr\":df_num.columns})\n",
    "\n",
    "#Goes through each \n",
    "for i,comp in enumerate(pca.components_):\n",
    "    importance[f\"PC{i+1}\"] = abs(comp)\n",
    "\n",
    "importance.drop(\"PC4\",axis=1,inplace=True)\n",
    "importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After plotting the points with respect to all of the combinations of the principle axes, the 2 groups of countries can easily be seen when looking at the PC1-PC2 and the PC1-PC3 plots. This relationship can be futher visualised using the 3D plot above. \n",
    "\n",
    "When trying to distinguish what the clusters represent, we need to think about the underlying priniciple axes as done above.\n",
    "\n",
    "PC1: A general danger level of the state, since it is strongly related to the murder, assault, and rape statistics, while also being related to urban population, but not by much.\n",
    "\n",
    "PC2: Tried to separate more rural states from more urban states but also taking into account their murder rates. So a more urban state with a high murder rate could land somewhere close to a rural state with a low murder rate.\n",
    "\n",
    "PC3: Tries to separate the types of crimes taking place, but favours the rape statistic more than others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime = [\"Murder\",\"Assault\",\"Rape\"]\n",
    "print(\"SHOWING PC1 TRENDS - Sum of all stats\")\n",
    "print(\"(TX):\",df_num_scaled.loc[\"TX\"].sum())\n",
    "print(\"(KS):\",df_num_scaled.loc[\"KS\"].sum())\n",
    "print(f\"{'':=^50}\")\n",
    "\n",
    "print(\"SHOWING PC2 TRENDS\")\n",
    "print(\"(CO):\",dict(df_num_scaled.loc[\"CO\",[\"UrbanPop\",\"Murder\"]]))\n",
    "print(\"(NV):\",dict(df_num_scaled.loc[\"NV\",[\"UrbanPop\",\"Murder\"]]))\n",
    "print(f\"{'':=^50}\")\n",
    "\n",
    "print(\"SHOWING PC3 TRENDS - Rape stats\")\n",
    "print(\"(OR):\",df_num_scaled.loc[\"OR\",\"Rape\"])\n",
    "print(\"(WA):\",df_num_scaled.loc[\"WA\",\"Rape\"])\n",
    "print(\"(DE):\",df_num_scaled.loc[\"DE\",\"Rape\"])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trends have been shown by picking certain states that have some sort of relationship to eachother. For example, to show the PC1 is the general risk level of a state, picking TX and KS, which are on opposite ends of the plot shows that TX has much more crime and a higher urban population than KS. which is what was expected.\n",
    "\n",
    "To varify PC2, CO and NV have been picked since they are close to eachother, however, even though NV has a higher urban population, the murder rate means that is is placed slightly more positive on the PC2 axis than CO. Which is what was expected.\n",
    "\n",
    "The varify PC3, the rape statisics of a few states were chosen. They were chosen because each state was slightly more negavtive than the last on the PC3 axis, Therefore, the rape stat reduced. Which was expected.\n",
    "\n",
    "Knowing this as looking at the 3D scatter plot, I can say that the main separation between the 2 clusters was along the PC2 axis, meaning that PC1 was the main factor in deciding which cluster a point was put into. Indicating that the clusters are sorted into states that are generally more dangerous and states that are generally safer.\n",
    "\n",
    "## Clustering - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T51",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1 (tags/v3.8.1:1b293b6, Dec 18 2019, 23:11:46) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5f4d0ae35e4bf29cc4a8c7f24047236fe139b58742fc51f1f31dad1d39118af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
